[control]
state_file = 'model/mortal.pth'
tensorboard_dir = 'log/tensorboard'

device = 'cuda:0'
enable_cudnn_benchmark = true
enable_amp = false

batch_size = 512
opt_step_every = 1

save_every = 800
test_every = 9600 # must be the mutiple of save_every

online = false

# using env `TRAIN_PLAY_PROFILE`
[train_play.default]
games = 800
log_dir = 'log/train_play'
boltzmann_epsilon = 0.005
boltzmann_temp = 0.05
repeats = 2

[test_play]
games = 2000
log_dir = 'log/test_play'

[dataset]
globs = [
	'dataset/tenhou_n22/*.json.gz',
	'dataset/tenhou_n21/*.json.gz',
	'dataset/tenhou_n20/*.json.gz',
	'dataset/tenhou_n19/*.json.gz',
	'dataset/tenhou_n18/*.json.gz',
	'dataset/tenhou_n17/*.json.gz',
	'dataset/tenhou_n16/*.json.gz',
	'dataset/tenhou_n15/*.json.gz',
	# 'dataset/tenhou_n14/*.json.gz', # invalid data
	'dataset/tenhou_n13/*.json.gz',
	'dataset/tenhou_n12/*.json.gz',
	'dataset/tenhou_n11/*.json.gz',
	'dataset/tenhou_n10/*.json.gz',
	'dataset/tenhou_n9/*.json.gz',
	'dataset/tenhou_n8/*.json.gz',
	'dataset/tenhou_n7/*.json.gz',
	'dataset/tenhou_n6/*.json.gz',
	'dataset/tenhou_n5/*.json.gz',
	'dataset/tenhou_n4/*.json.gz',
	'dataset/tenhou_n3/*.json.gz',
	'dataset/tenhou_n2/*.json.gz',
	'dataset/tenhou_n1/*.json.gz',
]
file_index = 'dataset/file_index.pth'
file_batch_size = 20
num_workers = 1
quality_threshold = 0

[env]
gamma = 1
# pts = [3.0, 1.5, 0.0, -4.5]
# 净点 = [15, 5, -5, -15]
# pts = [110.0, 55.0, 0.0, -195.0] # 豪3
# pts = [125.0, 60.0, -5.0, -210.0] # 豪3
# pts = [125.0, 60.0, -5.0, -210.0] # 豪3
pts = [1.0, 0.48, -0.04, -1.68] # 豪3

[resnet]
conv_channels = 192
num_blocks = 40
enable_bn = true
bn_momentum = 0.99

[cql]
min_q_weight = 5

[vlog]
beta_init = 1e-4
kld_target = 50

[freeze_bn]
mortal = true
oracle = true

[optim]
mortal_lr = 1e-4
oracle_lr = 1e-4
dqn_lr = 1e-4
beta_lr = 5e-4

[baseline]
device = 'cuda:0'
state_file = 'model/mortal-v6.5.pth'

[online.remote]
host = '127.0.0.1'
port = 5000

[online.server]
buffer_dir = 'log/buffer'
drain_dir = 'log/drain'
sample_reuse_rate = 0
sample_reuse_threshold = 0
capacity = 1600

[test]
stochastic_latent = false

[1v3]
games_per_iter = 800
iters = 10
log_dir = 'log/1v3'

[1v3.challenger]
device = 'cuda:0'
name = 'mortal'
state_file = 'model/mortal-v6.5.pth'
stochastic_latent = false
enable_amp = true
enable_rule_based_agari_guard = true

[1v3.champion]
device = 'cuda:0'
name = 'baseline'
state_file = 'model/mortal-v4.7.pth'
stochastic_latent = false
enable_amp = true
enable_rule_based_agari_guard = true

[1v3.akochan]
enabled = false
dir = '/path/to/akochan'
tactics = '/path/to/tactics.json'

[grp]
state_file = 'model/grp.pth'

[grp.network]
hidden_size = 64
num_layers = 2

[grp.control]
device = 'cuda:0'
enable_cudnn_benchmark = false
tensorboard_dir = 'log/tensorboard'

batch_size = 512
save_every = 2000
val_steps = 400

[grp.dataset]
train_globs = [
	'dataset/tenhou_n21/*.json.gz',
	'dataset/tenhou_n20/*.json.gz',
	'dataset/tenhou_n18/*.json.gz',
	'dataset/tenhou_n17/*.json.gz',
	'dataset/tenhou_n16/*.json.gz',
	'dataset/tenhou_n15/*.json.gz',
	'dataset/tenhou_n13/*.json.gz',
	'dataset/tenhou_n11/*.json.gz',
	'dataset/tenhou_n10/*.json.gz',
	'dataset/tenhou_n9/*.json.gz',
	'dataset/tenhou_n8/*.json.gz',
	'dataset/tenhou_n7/*.json.gz',
	'dataset/tenhou_n6/*.json.gz',
	'dataset/tenhou_n5/*.json.gz',
	'dataset/tenhou_n4/*.json.gz',
	'dataset/tenhou_n2/*.json.gz',
	'dataset/tenhou_n1/*.json.gz',
]
val_globs = [
	'dataset/tenhou_n22/*.json.gz',
	'dataset/tenhou_n19/*.json.gz',
	'dataset/tenhou_n12/*.json.gz',
	'dataset/tenhou_n3/*.json.gz',
]

# rebuild the index by delete this file
file_index = 'dataset/grp_file_index.pth'
file_batch_size = 50

train_num_workers = 0
val_num_workers = 0

[grp.optim]
lr = 1e-5
